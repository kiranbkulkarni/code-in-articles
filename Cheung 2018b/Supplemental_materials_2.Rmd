---
title: 'Supplemental Materials #2: Re-analysis of Study 14 (Monnot et al., 2011) in Yu et al. (2016)'
author: 'Mike W.-L. Cheung'
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output:
  pdf_document:
    toc: yes
    keep_tex: true
  html_document:
    keep_md: yes
    self_contained: yes
    theme: united
    toc: yes
  word_document:
    toc: yes
---

# Functions for the analysis
* This function is modified from the one in Yu et al. (2016) Supplemental Materials #1 with three key differences:
    1. The original function incorrectly uses SDs to represent variances to generate the bootstrap correlation matrices. This error is corrected here.
    2. The original function calls build.matrix() recursively. R will throw an error when it goes too deep in generating positive definite matrices. This revised function breaks it down into two functions so that there are no recursive calls.
    3. It adds a `nearPD` argument. If it is `TRUE`, it transforms the non-positive definite matrices into the near positive definite matrices with the `nearPD()` function in the `Matrix` package. If it is `FALSE`, new matrices will be generated until it is positive definite.

```{r, message=FALSE}
random.matrices <- function(point.matrix, reps, nearPD=FALSE) {
  ## SDs of the correlations
  SDs <- diag(point.matrix[,,2][lower.tri(point.matrix[,,2])])

  ## Convert the SDs to Variances
  sigma <- SDs^2
  
  ## Mean of correlation vector
  mu <- point.matrix[,,1][lower.tri(point.matrix[,,1])]
    
  build.matrix <- function(mu, sigma)  {
    vec <- mvrnorm(1, mu, Sigma=sigma )
    mat <- diag(1, nrow(point.matrix))
    mat[lower.tri(mat)] <- vec
    trans.mat <- t(mat)
    mat[upper.tri(mat)] <- trans.mat[upper.tri(trans.mat)]
    mat
  }
 
  ## Counter for NPD matrices
  countNPD <- 0
  
  ## Setup an array to store the generated correlation matrices
  output <- array(1, dim = c(nrow(point.matrix), ncol(point.matrix), reps))
  
  for (rep in 1:reps) { 
    mat <- build.matrix(mu=mu, sigma=sigma)
    
    ## Run nearPD==TRUE
    if (nearPD) {
      if (!is.positive.definite(mat)) {
        mat <- as.matrix(nearPD(mat, keepDiag = T)$mat)
        countNPD <- countNPD+1
      }
    } else {
      ## Run rearPD==FALSE
      while(!is.positive.definite(mat)) {
        mat <- build.matrix(mu=mu, sigma=sigma)
        countNPD <- countNPD+1
      } 
    }
    
    output[,,rep] <- mat
  }
  rownames(output) <- colnames(output) <- rownames(point.matrix)
  return(list(countTotal=(reps+countNPD), countNPD=countNPD, data=output))
}


## This function tests the biases of the generated data.
testBias <- function(point.matrix, reps, nearPD=FALSE) {
  ## Generate data
  mat <- random.matrices(point.matrix=point.matrix, reps=reps, nearPD=nearPD)
  
  ## Convert it into a list for ease of manipulations
  mat.list <- alply(mat$data, 3)

  ## Percentage of matrices is PD
  Percentage.NPD <- mat$countNPD/mat$countTotal*100
  
  ## Convert the list into a matrix of vectors to calculate mean and SD
  mat.vec <- t(sapply(mat.list, lav_matrix_vech, diagonal=FALSE))
  
  ## Mean of the correlation matrices
  mat.rc <- lav_matrix_vech_reverse(apply(mat.vec, 2, mean), diagonal = FALSE)
  diag(mat.rc) <- 1

  # Percentage bias of the mean
  Percentage.bias.rc <- (mat.rc-point.matrix[,,1])/point.matrix[,,1]*100
  # Remove Inf as NA
  Percentage.bias.rc[is.infinite(Percentage.bias.rc)] <- NA
  
  ## sd of the correlation matrices
  mat.sd <- lav_matrix_vech_reverse(apply(mat.vec, 2, sd), diagonal = FALSE)
  
  # Percentage bias of the SDs
  Percentage.bias.sd <- (mat.sd-point.matrix[,,2])/point.matrix[,,2]*100
  
  ## Bias of correlation among the generated correlation vectors
  CorAmongCor <- lav_matrix_vech(cor(mat.vec), diagonal = FALSE)

  list(summary=list(Percentage.NPD=Percentage.NPD, 
                    Percentage.bias.rc=Percentage.bias.rc,
                    Percentage.bias.sd=Percentage.bias.sd,
      Summary.percentage.bias.rc=summary(lav_matrix_vech(Percentage.bias.rc, diagonal = FALSE)),
      Summary.percentage.bias.sd=summary(lav_matrix_vech(Percentage.bias.sd, diagonal = FALSE)),
                    CorAmongCor=summary(CorAmongCor)), data=mat$data)
}

## This function is based on the one in Yu et al. Supplemental Materials #1
FIMASEM <- function(model, input.matrices, sample.nobs) {
  coefs.fits <- as.data.frame(t(sapply(1:dim(input.matrices)[3], function(i) {
    temp.sem <- sem(model=model, sample.nobs=sample.nobs,
                    sample.cov=input.matrices[,,i], warn=FALSE)
    c(coef(temp.sem), fitMeasures(temp.sem))})))
}
```

# Input data
```{r, message=FALSE}
## Required packages
lib2install <- c("plyr", "matrixcalc", "lavaan", "MASS", "psych", "Matrix")

## Install them automatically if they are not available on your computer
for (i in lib2install) {
  if (!(i %in% rownames(installed.packages()))) install.packages(i)
}

## Libraries used in the analysis
library(plyr)
library(matrixcalc)
library(lavaan)
library(MASS)
library(psych)  
library(Matrix)

## Set the seed for replication
set.seed(927037462)

## Large bootstrap replications to minimize simulation error
reps <- 10000

## Sample size
sample.nobs <- 200
```

* It is important to note that standard deviations (SDs) were incorrectly used to represent variances to generate the bootstrap correlation matrices in the `random.matrices.univarite()` in Yu's et al. Supplemental Materials #1. This error is corrected here.
    + `inputmatrices`: SD matrix used in Yu et al. (2016)
```{r}
rc <- matrix(c(1,.47,.12,.20,.24,.00,
               .47,1,.11,.18,.11,-.04,
               .12,.11,1,.70,.73,.40,
               .20,.18,.70,1,.56,.22,
               .24,.11,.73,.56,1,.41,
               .00,-.04,.40,.22,.41,1), nr=6)
sd <- matrix(c(.00,.30,.20,.14,.23,.20,
			  .30,.00,.22,.17,.22,.18,
			  .20,.22,.00,.19,.16,.20,
			  .14,.17,.19,.00,.26,.16,
			  .23,.22,.16,.26,0,.24,
			  .20,.18,.20,.16,.24,0), nr=6)
input.matrices <- array(0, dim=c(nrow(rc), ncol(rc),2))
input.matrices[,,1] <- rc
input.matrices[,,2] <- sd
dimnames(input.matrices) <- list(c('ORGC','JSAT','PROUATT','INSTRU','UCOM','UPART'),
                                 c('ORGC','JSAT','PROUATT','INSTRU','UCOM','UPART'),
                                 c("rc", "sd"))
input.matrices
```

# Checking the generated correlation matrices

## Replace NPD correlation matrices with newly generated matrices (replacement method)
```{r}
replacement.data <- testBias(point.matrix=input.matrices, reps=reps, nearPD=FALSE)
replacement.data$summary
```

## Replace NPD correlation matrices with nearPD matrices (nearPD method)
```{r}
nearPD.data <- testBias(point.matrix=input.matrices, reps=reps, nearPD=TRUE)
nearPD.data$summary
```

# Re-analysis of FIMASEM
```{r}
model <- "UPART ~ UCOM
          UCOM ~ ORGC + JSAT + PROUATT + INSTRU
          ORGC ~ JSAT
          PROUATT ~ INSTRU"
```

## Analysis of the bootstrap data with replacement method
```{r}
fit <- FIMASEM(model, replacement.data$data, sample.nobs=sample.nobs)

cfi.fit <- ifelse(fit$cfi>0.9, yes=1, no=0)
srmr.fit <- ifelse(fit$srmr<0.1, yes=1, no=0)
rmsea.fit <- ifelse(fit$rmsea<0.05, yes=1, no=0)

cat("Percentage of CFA >.9:", mean(cfi.fit)*100)
cat("Percentage of SRMR < .1:", mean(srmr.fit)*100)
cat("Percentage of sample RMSEA < .05:", mean(rmsea.fit)*100)

describe(fit[, c("chisq", "df", "cfi", "rmsea", "srmr")], 
         trim=0, quant=c(.10, .25, .50, .75, .90))
pairs.panels(fit[, c("chisq", "cfi", "rmsea", "srmr")], hist.col="yellow")

save.image("SuppMat2.RData")

sessionInfo()
```
